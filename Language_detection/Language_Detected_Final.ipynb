{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82612571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imported basic Liberaries used for any algorithm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fa6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "dataset = pd.read_csv('language_detection.csv')\n",
    "train_data = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5fb129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import re liberary for removing extra elements from the strings or to tokenize the string\n",
    "import re\n",
    "#  nitk liberary for Reduce words to their stems and to \n",
    "# remove some common words like the,in,is etc from the string which are of no use\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Stopword remove unnecessary words like the,is,not etc\n",
    "from nltk.corpus import stopwords\n",
    "# PorterStemmer reduces word to their stems\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7954e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus is a list containing all filtered strings\n",
    "corpus = []\n",
    "for i in range(0, 22000):\n",
    "    review = dataset['Text'][i]\n",
    "    review = review.lower() # Convert uppercase letter in small case letters\n",
    "    review = review.split() # spilts each word and make array\n",
    "    ps = PorterStemmer() \n",
    "    review = [ps.stem(word) for word in review] # passes each word from ps.stem() to reduce words to there stems\n",
    "    review = ' '.join(review) # join words to recombine string\n",
    "    corpus.append(review) # append filtered string in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98882f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import CountVectorizer from sklearn liberary used to convert strings into BOWs array\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 3000)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd49ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 17 19 ... 16  1 14]\n",
      "{4: 'Estonian', 17: 'Swedish', 19: 'Thai', 18: 'Tamil', 2: 'Dutch', 8: 'Japanese', 20: 'Turkish', 10: 'Latin', 21: 'Urdu', 7: 'Indonesian', 12: 'Portugese', 5: 'French', 1: 'Chinese', 9: 'Korean', 6: 'Hindi', 16: 'Spanish', 13: 'Pushto', 11: 'Persian', 14: 'Romanian', 15: 'Russian', 3: 'English', 0: 'Arabic'}\n"
     ]
    }
   ],
   "source": [
    "y_y = dataset.iloc[:,-1].values\n",
    "#  LabelEncoding used to convert column into vectors without increasing the number of column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_y)\n",
    "print(y)\n",
    "\n",
    "#  dictionary to save each language where key value of languages is equal to there LabelEncoding vectored value\n",
    "dic_result = {}\n",
    "for i in range(len(y)):\n",
    "    dic_result[y[i]] = y_y[i]\n",
    "\n",
    "print(dic_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aadc96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses Knn neighbourer algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac5655fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test string\n",
    "text_ = \"В то время как центральное агентство Федеральной обороны США организовывает постоянные рейды, чтобы выследить его, Джек Стил Стил Джо Джек — андроид-охотник за головами, специализирующийся на огнестрельном оружии для гангстерских лимеров, чтобы вернуть своих товарищей Я нанял его, поэтому он, кажется, склонен наряжаться. Когда Железная Башня, а позже и люди генерала Марти присоединяются и пытаются помешать ей спасти союзника из Железной Башни, они разрушают ее с помощью Вайолет.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229eb2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В то время как центральное агентство Федеральной обороны США организовывает постоянные рейды, чтобы выследить его, Джек Стил Стил Джо Джек — андроид-охотник за головами, специализирующийся на огнестрельном оружии для гангстерских лимеров, чтобы вернуть своих товарищей Я нанял его, поэтому он, кажется, склонен наряжаться. Когда Железная Башня, а позже и люди генерала Марти присоединяются и пытаются помешать ей спасти союзника из Железной Башни, они разрушают ее с помощью Вайолет.\n",
      "в то время как центральное агентство федеральной обороны сша организовывает постоянные рейды чтобы выследить его джек стил стил джо джек — андроид-охотник за головами специализирующийся на огнестрельном оружии для гангстерских лимеров чтобы вернуть своих товарищей я нанял его поэтому он кажется склонен наряжаться. когда железная башня а позже и люди генерала марти присоединяются и пытаются помешать ей спасти союзника из железной башни они разрушают ее с помощью вайолет.\n"
     ]
    }
   ],
   "source": [
    "print(text_)\n",
    "text_ = re.sub(r'[!@#$(),\"%^*?:;~`0-9]', ' ', text_)\n",
    "text_ = re.sub(r'[[]]', ' ', text_)\n",
    "text_ = text_.lower()\n",
    "text_ = text_.split()\n",
    "text_ = [ps.stem(word) for word in text_]\n",
    "text_ = ' '.join(text_)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b8cd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [text_]\n",
    "X_test = cv.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "823ad345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\n",
      "Russian\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test.toarray())\n",
    "print(y_pred)\n",
    "print(dic_result[y_pred[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
